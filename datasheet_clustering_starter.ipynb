{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7523c467",
   "metadata": {},
   "source": [
    "# Datasheet Clustering Starter Notebook\n",
    "This notebook guides you through building a prototype that ingests component datasheet PDFs, extracts text & numeric features, and clusters similar products.\n",
    "\n",
    "> **Setup**: Create a virtual environment and install required packages first:\n",
    "> ```bash\n",
    "> pip install pymupdf pdfplumber sentence-transformers faiss-cpu hdbscan umap-learn plotly tqdm\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, pathlib\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan, umap\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e79acf6",
   "metadata": {},
   "source": [
    "## 1. Helper: PDF extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf(path):\n",
    "    \"\"\"Return raw text and list of DataFrames extracted from tables.\"\"\"\n",
    "    doc = fitz.open(path)\n",
    "    text_chunks, tables = [], []\n",
    "    for page in doc:\n",
    "        text_chunks.append(page.get_text(\"text\"))\n",
    "        try:\n",
    "            tbls = page.find_tables()[0]\n",
    "            tables.extend(tbls)\n",
    "        except Exception:\n",
    "            pass  # no tables on this page\n",
    "    return \"\\n\".join(text_chunks), tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c4551",
   "metadata": {},
   "source": [
    "## 2. Parse numeric specs (placeholder)\n",
    "Edit `SPEC_REGEXES` to match the parameters you care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ff08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEC_REGEXES = {\n",
    "    \"vdd_max\": r\"V[Dd][Dd].{0,20}?([0-9]+\\.?[0-9]*)\\s*[Vv]\",\n",
    "    \"gain_db\": r\"[Gg]ain.*?([0-9]+\\.?[0-9]*)\\s*dB\",\n",
    "}\n",
    "\n",
    "def parse_specs(text):\n",
    "    specs = {}\n",
    "    for key, pattern in SPEC_REGEXES.items():\n",
    "        m = re.search(pattern, text)\n",
    "        if m:\n",
    "            specs[key] = float(m.group(1))\n",
    "    return specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd8148",
   "metadata": {},
   "source": [
    "## 3. Build vector: text embedding + numeric vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c99301",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "NUM_FEATURES = list(SPEC_REGEXES.keys())\n",
    "\n",
    "def build_vector(text, numeric_dict):\n",
    "    text_vec = model.encode(text, normalize_embeddings=True)\n",
    "    numeric = np.array([numeric_dict.get(k, np.nan) for k in NUM_FEATURES])\n",
    "    numeric = np.nan_to_num(numeric)  # simple imputation\n",
    "    return np.hstack([text_vec, numeric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84092cc",
   "metadata": {},
   "source": [
    "## 4. Clustering utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_vectors(X, min_cluster_size=5):\n",
    "    # Dimensionality reduction (optional but speeds HDBSCAN)\n",
    "    red = umap.UMAP(n_components=10, random_state=42).fit_transform(X)\n",
    "    clust = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size).fit(red)\n",
    "    return clust.labels_, red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f97e85e",
   "metadata": {},
   "source": [
    "## 5. End‑to‑end example\n",
    "Drop a few sample PDFs inside a folder (e.g., `./sample_datasheets`) and run the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26497106",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_DIR = './sample_datasheets'  # change to your folder\n",
    "rows = []\n",
    "for pdf_path in glob.glob(os.path.join(PDF_DIR, '*.pdf')):\n",
    "    text, tables = extract_pdf(pdf_path)\n",
    "    specs = parse_specs(text)\n",
    "    vec = build_vector(text, specs)\n",
    "    rows.append({'file': os.path.basename(pdf_path), 'vector': vec, 'specs': specs})\n",
    "\n",
    "# Build feature matrix\n",
    "X = np.vstack([r['vector'] for r in rows])\n",
    "\n",
    "# Cluster\n",
    "labels, red = cluster_vectors(X)\n",
    "for r, lab in zip(rows, labels):\n",
    "    r['cluster'] = int(lab)\n",
    "\n",
    "# Visualise\n",
    "vis_df = pd.DataFrame({\n",
    "    'x': red[:,0],\n",
    "    'y': red[:,1],\n",
    "    'cluster': labels,\n",
    "    'file': [r['file'] for r in rows]\n",
    "})\n",
    "fig = px.scatter(vis_df, x='x', y='y', color='cluster', hover_name='file', title='Datasheet Clusters')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ddc444",
   "metadata": {},
   "source": [
    "---\n",
    "### Next steps\n",
    "- **Vector store**: Persist embeddings in FAISS/Qdrant for similarity search.\n",
    "- **Better spec parsing**: Use `pdfplumber` tables or NLP entity extraction for robust numeric features.\n",
    "- **Web service**: Wrap logic in FastAPI + React/Streamlit front‑end.\n",
    "- **Evaluation**: Silhouette score, manual cluster inspection dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bafd368",
   "metadata": {},
   "source": [
    "## 6. Advanced spec parsing\n",
    "This cell shows **two complementary techniques** to pull numeric specs more robustly:\n",
    "1. **Table extraction** with `pdfplumber` & heuristics for unit conversion.\n",
    "2. **Regex on sentences** for specs hidden in prose.\n",
    "\n",
    "> **Tip**: Extend `TABLE_HEADERS` and `SENTENCE_PATTERNS` dictionaries with the parameters important to your parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber, itertools\n",
    "UNIT_MAP = {\n",
    "    'v': 1.0, 'mv': 1e-3, 'kv': 1e3,\n",
    "    'a': 1.0, 'ma': 1e-3, 'ua': 1e-6,\n",
    "    'db': 1.0\n",
    "}\n",
    "\n",
    "TABLE_HEADERS = {\n",
    "    'vdd_max': ['vdd', 'supply voltage', 'vcc'],\n",
    "    'gain_db': ['gain', 'power gain'],\n",
    "    'freq_max_hz': ['frequency range', 'f_max'],\n",
    "}\n",
    "\n",
    "SENTENCE_PATTERNS = {\n",
    "    'noise_figure_db': r\"noise\\s*figure[^0-9]*([0-9]+\\.?[0-9]*)\\s*dB\",\n",
    "}\n",
    "\n",
    "def _to_float(val):\n",
    "    try:\n",
    "        return float(str(val).strip())\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def parse_tables_plumber(path):\n",
    "    out = {}\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            for table in page.extract_tables():\n",
    "                for row in table:\n",
    "                    for k, aliases in TABLE_HEADERS.items():\n",
    "                        if any(a.lower() in str(row[0]).lower() for a in aliases):\n",
    "                            # find first numeric in row\n",
    "                            num_cells = [c for c in row[1:] if re.search(r\"[0-9]\", str(c))]\n",
    "                            if num_cells:\n",
    "                                val_str = re.findall(r\"([0-9]+\\.?[0-9]*)\\s*([kmu]?v|db|a)?\", str(num_cells[0].lower()))\n",
    "                                if val_str:\n",
    "                                    mag, unit = val_str[0]\n",
    "                                    mag = float(mag)\n",
    "                                    factor = UNIT_MAP.get(unit, 1.0)\n",
    "                                    out[k] = mag * factor\n",
    "    return out\n",
    "\n",
    "def advanced_parse_specs(text, path):\n",
    "    specs = {}\n",
    "    # sentence regex pass\n",
    "    for k, pat in SENTENCE_PATTERNS.items():\n",
    "        m = re.search(pat, text, re.I)\n",
    "        if m:\n",
    "            specs[k] = _to_float(m.group(1))\n",
    "    # table pass\n",
    "    table_specs = parse_tables_plumber(path)\n",
    "    specs.update(table_specs)\n",
    "    return specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c58a9e2",
   "metadata": {},
   "source": [
    "## 7. FAISS similarity search\n",
    "After clustering, you may want to retrieve the most similar products to a query part or to a free‑text description. This cell builds an **in‑memory FAISS index** over your vectors and shows how to query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b783882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Build index (run after you've created X)\n",
    "d = X.shape[1]\n",
    "index = faiss.IndexFlatIP(d)  # inner‑product for cosine (vectors must be L2‑normalised!)\n",
    "index.add(X.astype('float32'))\n",
    "\n",
    "def search(query_text, k=5):\n",
    "    q_vec = model.encode(query_text, normalize_embeddings=True)\n",
    "    q_vec = np.hstack([q_vec, np.zeros(len(NUM_FEATURES))])  # pad numeric zeros\n",
    "    D, I = index.search(np.expand_dims(q_vec.astype('float32'), 0), k)\n",
    "    return [(rows[i]['file'], float(D[0][j])) for j, i in enumerate(I[0])]\n",
    "\n",
    "print(search(\"high gain 5 GHz amplifier\", k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b73a1a",
   "metadata": {},
   "source": [
    "## 8. FastAPI micro‑service\n",
    "The following cell spins up a **FastAPI** app that lets you `POST` a PDF and receive the top‑N similar products. Run with `uvicorn main:app --reload` (or use the `__main__` block below for inline launch).\n",
    "\n",
    "> **Install**:\n",
    "> ```bash\n",
    "> pip install fastapi uvicorn python-multipart\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27765cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, File, UploadFile\n",
    "import tempfile, shutil\n",
    "\n",
    "app = FastAPI(title='Datasheet similarity API')\n",
    "\n",
    "@app.post('/search')\n",
    "async def search_pdf(file: UploadFile = File(...), top_k: int = 5):\n",
    "    # save to temp\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:\n",
    "        shutil.copyfileobj(file.file, tmp)\n",
    "        tmp_path = tmp.name\n",
    "    text, tables = extract_pdf(tmp_path)\n",
    "    specs = advanced_parse_specs(text, tmp_path)\n",
    "    vec = build_vector(text, specs)\n",
    "    D, I = index.search(np.expand_dims(vec.astype('float32'), 0), top_k)\n",
    "    hits = [{'file': rows[i]['file'], 'score': float(D[0][j])} for j, i in enumerate(I[0])]\n",
    "    return {'hits': hits, 'specs': specs}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host='0.0.0.0', port=8000)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
